{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs = 1, n_layers = 1, n_neurons_per_layer = None, leaky_slope = 0.01):\n",
    "        if n_layers > 0:\n",
    "            try:\n",
    "                assert n_neurons_per_layer is not None\n",
    "                assert len(n_neurons_per_layer) == n_layers\n",
    "            except Exception as e:\n",
    "                raise ValueError(\"provide numbers of neurons per layer defined by n_layers\")\n",
    "        \n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        if n_layers >= 1:\n",
    "            stack = [ nn.Linear( n_inputs, n_neurons_per_layer[0] ) ]\n",
    "            for k in range(n_layers-1):\n",
    "                stack += [ nn.LeakyReLU(leaky_slope), nn.Linear(n_neurons_per_layer[k], n_neurons_per_layer[k+1]) ]\n",
    "            stack += [ nn.LeakyReLU(leaky_slope), nn.Linear(n_neurons_per_layer[n_layers-1], n_outputs) ]\n",
    "        else:\n",
    "            stack = [ nn.Linear( n_inputs, n_outputs ) ]\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential( *stack )\n",
    "        self.prob_predictor = nn.Sigmoid()\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        for name, param in self.named_parameters():\n",
    "            param.data.uniform_(-0.5,0.5)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return self.prob_predictor(logits)\n",
    "    \n",
    "    def loss_fn(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        return self.loss(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        dataset = [ json.loads(line, parse_int = str) for line in f ]\n",
    "    return dataset\n",
    "\n",
    "def get_vectorizer( texts: list[str], *, method=\"countvectorize\", **kwargs ) -> CountVectorizer:\n",
    "    \"\"\"From a list of texts, output an appropriate vectorizer either using CountVectorizer or TF-IDF depending on method argument. \n",
    "\n",
    "     Args:\n",
    "         texts (list[str]): list of strings, each item corresponding to a text.\n",
    "         method (str, optional): Method to select features. Defaults to \"count-vectorizer\".\n",
    "         **kwargs: kwarg arguments to pass to Vectorizer classes of sklearn.\n",
    "    Raises:\n",
    "        ValueError: If passing an non-specified method of text feature extraction\n",
    "\n",
    "     Returns:\n",
    "         pd.DataFrame: dataframe of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    #We want single digits to be tokenized. This regex considers everything as a token except whitespace.\n",
    "    kwargs['token_pattern'] = r'\\S+' \n",
    "    if method == \"countvectorize\":\n",
    "        vectorizer = CountVectorizer(**kwargs)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"{method} is not a supported method.\")\n",
    "    #Use texts to initialize vocabulary of vectorizer\n",
    "    vectorizer.fit(texts)\n",
    "    return vectorizer\n",
    "\n",
    "def sentencify(text: list) -> str:\n",
    "    sentence = \" \".join(text) \n",
    "    return sentence\n",
    "\n",
    "def loss(clf, X, y):\n",
    "    probs = clf.predict_log_proba(X)\n",
    "    y0 = probs[:,1]\n",
    "    y1 = probs[:,0]\n",
    "    loss = -y*y0 - (1-y)*y1\n",
    "    loss = loss.sum()/y.size\n",
    "    return loss\n",
    "\n",
    "def balanced_acc(clf, X, y):\n",
    "    y_pred = clf.predict(X)\n",
    "    return balanced_accuracy_score(y, y_pred, adjusted=True)\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = load_dataset(\"domain1_train_data.json\")\n",
    "datatexts_1 = [ sentencify(instance['text']) for instance in dataset_1 ]\n",
    "\n",
    "dataset_2 = load_dataset(\"domain2_train_data.json\")\n",
    "datatexts_2 = [ sentencify(instance['text']) for instance in dataset_2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no features: 110827\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = get_vectorizer( texts = datatexts_1 + datatexts_2,\n",
    "                                    method='tfidf',\n",
    "                                    use_idf=True,\n",
    "                                    ngram_range=(1,3),\n",
    "                                    max_df=0.995, #Ignore vocabulary appearing too frequently, probably words like \"is\", \"are\", \"and\", \"this\" etc.\n",
    "                                    min_df=10, #Ignore vocabulary that is too infrequent, as this may lead to low prediction accuracy.\n",
    "                                    )\n",
    "print(f\"no features: {tfidf_vectorizer.get_feature_names_out().size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0:\"domain1_ai\",\n",
    "          1:\"domain1_human\",\n",
    "          2:\"domain2_ai\",\n",
    "          3:\"domain2_human\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tfidf_vectorizer.transform( datatexts_1 + datatexts_2 ).toarray()\n",
    "y = np.array([1]*2500 + [0]*2500 + [3]*1500 + [2]*11500)\n",
    "lengths = np.zeros( (18000, 2) )\n",
    "for i in range(5000):\n",
    "    for k in range(2):\n",
    "        lengths[i,k] = len(dataset_1[i]['text'])**(k+1)\n",
    "for i in range(13000):\n",
    "    for k in range(2):\n",
    "        lengths[5000+i,k] = len(dataset_2[i]['text'])**(k+1)\n",
    "lengths[:,0] /= lengths.max(axis=0)[0]\n",
    "lengths[:,1] /= lengths.max(axis=0)[1]\n",
    "X = np.hstack( (X, lengths) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vector = np.zeros((18000, 4))\n",
    "y_vector[(np.arange(18000), y)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"temp_selector_clf.mdl\", \"rb\") as f:\n",
    "    selector_clf = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26799"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectFromModel(selector_clf, prefit=True)\n",
    "selector.fit(X, y)\n",
    "selector.get_feature_names_out().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = selector.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X.astype(float), y_vector.astype(float),\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=y\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "X_train = X_train.to(mps_device)\n",
    "y_train = y_train.to(mps_device)\n",
    "X_test = X_test.to(mps_device)\n",
    "y_test = y_test.to(mps_device)\n",
    "train_dataloader = DataLoader(list(zip(X_train,y_train)), shuffle=True, batch_size=X_train.shape[0]//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=26799, out_features=500, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=500, out_features=100, bias=True)\n",
       "    (3): LeakyReLU(negative_slope=0.01)\n",
       "    (4): Linear(in_features=100, out_features=4, bias=True)\n",
       "  )\n",
       "  (prob_predictor): Softmax(dim=1)\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = NeuralNetwork( X_train.shape[1], 4, 2, [500, 100] )\n",
    "model.prob_predictor = nn.Softmax(dim=1)\n",
    "model.loss = nn.CrossEntropyLoss()\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(n_epochs, learning_rate, weight_decay):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    for epoch in range(n_epochs):\n",
    "        mean_trainloss = 0\n",
    "        num_trainbatches = len(train_dataloader)\n",
    "        \n",
    "        #Begin training\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss_fn(X_batch, y_batch)\n",
    "            mean_trainloss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        mean_trainloss /= num_trainbatches\n",
    "        train_losses += [ mean_trainloss ]\n",
    "        \n",
    "        #Begin evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            testloss = model.loss_fn(X_test, y_test).item()\n",
    "            test_losses += [testloss]\n",
    "            y_pred = model.predict(X_test).round()\n",
    "            y_pred = y_pred.cpu().detach().numpy().reshape(-1)\n",
    "            accuracy = balanced_accuracy_score(y_test.cpu().detach().numpy().reshape(-1), y_pred)\n",
    "            test_accuracy += [accuracy]\n",
    "            \n",
    "            y_pred = model.predict(X_train).round()\n",
    "            y_pred = y_pred.cpu().detach().numpy().reshape(-1)\n",
    "            accuracy = balanced_accuracy_score(y_train.cpu().detach().numpy().reshape(-1), y_pred)\n",
    "            train_accuracy += [accuracy]\n",
    "        print(f'Finished epoch {epoch}, latest trainloss {np.round(mean_trainloss,4)}, testloss {np.round( testloss, 4 )}, test accuracy {np.round(test_accuracy[-1],4)}')\n",
    "    return train_losses, test_losses, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 19.6509, testloss 0.7277, test accuracy 0.8461\n",
      "Finished epoch 1, latest trainloss 1.5488, testloss 1.2683, test accuracy 0.8863\n",
      "Finished epoch 2, latest trainloss 0.6891, testloss 0.3336, test accuracy 0.915\n",
      "Finished epoch 3, latest trainloss 0.151, testloss 0.2387, test accuracy 0.9394\n",
      "Finished epoch 4, latest trainloss 0.0495, testloss 0.2337, test accuracy 0.9539\n",
      "Finished epoch 5, latest trainloss 0.0151, testloss 0.2824, test accuracy 0.9475\n",
      "Finished epoch 6, latest trainloss 0.0058, testloss 0.2942, test accuracy 0.9523\n",
      "Finished epoch 7, latest trainloss 0.0028, testloss 0.3004, test accuracy 0.9542\n",
      "Finished epoch 8, latest trainloss 0.0021, testloss 0.3196, test accuracy 0.9547\n",
      "Finished epoch 9, latest trainloss 0.0013, testloss 0.3378, test accuracy 0.9541\n"
     ]
    }
   ],
   "source": [
    "trains += [ train_epochs(n_epochs=10, learning_rate=0.1, weight_decay=0.0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 0.0009, testloss 0.3444, test accuracy 0.9506\n",
      "Finished epoch 1, latest trainloss 0.001, testloss 0.3567, test accuracy 0.9487\n",
      "Finished epoch 2, latest trainloss 0.0014, testloss 0.3376, test accuracy 0.9503\n",
      "Finished epoch 3, latest trainloss 0.0018, testloss 0.3464, test accuracy 0.9474\n",
      "Finished epoch 4, latest trainloss 0.0022, testloss 0.3429, test accuracy 0.9486\n",
      "Finished epoch 5, latest trainloss 0.0025, testloss 0.306, test accuracy 0.9494\n",
      "Finished epoch 6, latest trainloss 0.0029, testloss 0.3147, test accuracy 0.9492\n",
      "Finished epoch 7, latest trainloss 0.0037, testloss 0.3107, test accuracy 0.948\n",
      "Finished epoch 8, latest trainloss 0.0041, testloss 0.2815, test accuracy 0.9494\n",
      "Finished epoch 9, latest trainloss 0.0048, testloss 0.2569, test accuracy 0.9497\n",
      "Finished epoch 10, latest trainloss 0.0054, testloss 0.2517, test accuracy 0.9507\n",
      "Finished epoch 11, latest trainloss 0.0062, testloss 0.2649, test accuracy 0.9482\n",
      "Finished epoch 12, latest trainloss 0.0067, testloss 0.2358, test accuracy 0.951\n",
      "Finished epoch 13, latest trainloss 0.008, testloss 0.2443, test accuracy 0.9493\n",
      "Finished epoch 14, latest trainloss 0.0091, testloss 0.2499, test accuracy 0.9456\n",
      "Finished epoch 15, latest trainloss 0.0099, testloss 0.2166, test accuracy 0.9493\n",
      "Finished epoch 16, latest trainloss 0.0099, testloss 0.2324, test accuracy 0.949\n",
      "Finished epoch 17, latest trainloss 0.0106, testloss 0.2374, test accuracy 0.9459\n",
      "Finished epoch 18, latest trainloss 0.0117, testloss 0.2108, test accuracy 0.9505\n",
      "Finished epoch 19, latest trainloss 0.0127, testloss 0.2085, test accuracy 0.95\n"
     ]
    }
   ],
   "source": [
    "trains += [ train_epochs(n_epochs=20, learning_rate=0.01, weight_decay=0.001) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 0.0141, testloss 0.2151, test accuracy 0.9481\n",
      "Finished epoch 1, latest trainloss 0.0132, testloss 0.2081, test accuracy 0.9491\n",
      "Finished epoch 2, latest trainloss 0.0136, testloss 0.2077, test accuracy 0.9486\n",
      "Finished epoch 3, latest trainloss 0.014, testloss 0.204, test accuracy 0.9495\n",
      "Finished epoch 4, latest trainloss 0.0144, testloss 0.2097, test accuracy 0.947\n",
      "Finished epoch 5, latest trainloss 0.0146, testloss 0.2065, test accuracy 0.9486\n",
      "Finished epoch 6, latest trainloss 0.0148, testloss 0.2083, test accuracy 0.9484\n",
      "Finished epoch 7, latest trainloss 0.0148, testloss 0.2112, test accuracy 0.9483\n",
      "Finished epoch 8, latest trainloss 0.0152, testloss 0.2174, test accuracy 0.9463\n",
      "Finished epoch 9, latest trainloss 0.015, testloss 0.2037, test accuracy 0.9491\n",
      "Finished epoch 10, latest trainloss 0.0156, testloss 0.2184, test accuracy 0.9462\n",
      "Finished epoch 11, latest trainloss 0.0154, testloss 0.2089, test accuracy 0.9483\n",
      "Finished epoch 12, latest trainloss 0.0155, testloss 0.2089, test accuracy 0.9471\n",
      "Finished epoch 13, latest trainloss 0.0158, testloss 0.2105, test accuracy 0.9473\n",
      "Finished epoch 14, latest trainloss 0.0157, testloss 0.2088, test accuracy 0.9492\n",
      "Finished epoch 15, latest trainloss 0.0157, testloss 0.218, test accuracy 0.9472\n",
      "Finished epoch 16, latest trainloss 0.0158, testloss 0.2066, test accuracy 0.9498\n",
      "Finished epoch 17, latest trainloss 0.0157, testloss 0.2234, test accuracy 0.9455\n",
      "Finished epoch 18, latest trainloss 0.016, testloss 0.2043, test accuracy 0.9484\n",
      "Finished epoch 19, latest trainloss 0.0161, testloss 0.2175, test accuracy 0.9477\n",
      "Finished epoch 20, latest trainloss 0.0158, testloss 0.2144, test accuracy 0.9481\n",
      "Finished epoch 21, latest trainloss 0.016, testloss 0.2143, test accuracy 0.9473\n",
      "Finished epoch 22, latest trainloss 0.0161, testloss 0.2094, test accuracy 0.9473\n",
      "Finished epoch 23, latest trainloss 0.0162, testloss 0.219, test accuracy 0.946\n",
      "Finished epoch 24, latest trainloss 0.0163, testloss 0.2187, test accuracy 0.9467\n",
      "Finished epoch 25, latest trainloss 0.0164, testloss 0.2164, test accuracy 0.9473\n",
      "Finished epoch 26, latest trainloss 0.0164, testloss 0.2096, test accuracy 0.9481\n",
      "Finished epoch 27, latest trainloss 0.0165, testloss 0.2235, test accuracy 0.9438\n",
      "Finished epoch 28, latest trainloss 0.0165, testloss 0.2088, test accuracy 0.9485\n",
      "Finished epoch 29, latest trainloss 0.0165, testloss 0.2223, test accuracy 0.9443\n",
      "Finished epoch 30, latest trainloss 0.0168, testloss 0.2086, test accuracy 0.9484\n",
      "Finished epoch 31, latest trainloss 0.0168, testloss 0.2355, test accuracy 0.9408\n",
      "Finished epoch 32, latest trainloss 0.017, testloss 0.2052, test accuracy 0.9485\n",
      "Finished epoch 33, latest trainloss 0.0168, testloss 0.2186, test accuracy 0.9447\n",
      "Finished epoch 34, latest trainloss 0.0168, testloss 0.2112, test accuracy 0.9474\n",
      "Finished epoch 35, latest trainloss 0.0168, testloss 0.2192, test accuracy 0.9445\n",
      "Finished epoch 36, latest trainloss 0.0171, testloss 0.2188, test accuracy 0.9455\n",
      "Finished epoch 37, latest trainloss 0.0175, testloss 0.215, test accuracy 0.946\n",
      "Finished epoch 38, latest trainloss 0.0168, testloss 0.2102, test accuracy 0.9474\n",
      "Finished epoch 39, latest trainloss 0.0171, testloss 0.2151, test accuracy 0.946\n",
      "Finished epoch 40, latest trainloss 0.0173, testloss 0.2134, test accuracy 0.9451\n",
      "Finished epoch 41, latest trainloss 0.0172, testloss 0.2147, test accuracy 0.9462\n",
      "Finished epoch 42, latest trainloss 0.0172, testloss 0.207, test accuracy 0.9474\n",
      "Finished epoch 43, latest trainloss 0.0174, testloss 0.2148, test accuracy 0.9454\n",
      "Finished epoch 44, latest trainloss 0.0173, testloss 0.2101, test accuracy 0.9461\n",
      "Finished epoch 45, latest trainloss 0.0176, testloss 0.2122, test accuracy 0.9445\n",
      "Finished epoch 46, latest trainloss 0.0176, testloss 0.205, test accuracy 0.947\n",
      "Finished epoch 47, latest trainloss 0.018, testloss 0.2233, test accuracy 0.9419\n",
      "Finished epoch 48, latest trainloss 0.0176, testloss 0.2074, test accuracy 0.9456\n",
      "Finished epoch 49, latest trainloss 0.0176, testloss 0.2138, test accuracy 0.9435\n"
     ]
    }
   ],
   "source": [
    "trains += [ train_epochs(n_epochs=50, learning_rate=0.001, weight_decay=0.001) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now reshuffle the data and re-train as it is likely overfitting to the current training data\n",
    "X_train, X_test, y_train, y_test = train_test_split( X.astype(float), y_vector.astype(float),\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=2,\n",
    "                                                    stratify=y\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "X_train = X_train.to(mps_device)\n",
    "y_train = y_train.to(mps_device)\n",
    "X_test = X_test.to(mps_device)\n",
    "y_test = y_test.to(mps_device)\n",
    "train_dataloader = DataLoader(list(zip(X_train,y_train)), shuffle=True, batch_size=X_train.shape[0]//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 0.0561, testloss 0.0478, test accuracy 0.9916\n",
      "Finished epoch 1, latest trainloss 0.0505, testloss 0.0489, test accuracy 0.9909\n",
      "Finished epoch 2, latest trainloss 0.0463, testloss 0.0492, test accuracy 0.9906\n",
      "Finished epoch 3, latest trainloss 0.043, testloss 0.0495, test accuracy 0.9909\n",
      "Finished epoch 4, latest trainloss 0.0401, testloss 0.0504, test accuracy 0.9903\n",
      "Finished epoch 5, latest trainloss 0.0375, testloss 0.0511, test accuracy 0.9901\n",
      "Finished epoch 6, latest trainloss 0.0354, testloss 0.0516, test accuracy 0.9901\n",
      "Finished epoch 7, latest trainloss 0.0335, testloss 0.0523, test accuracy 0.9901\n",
      "Finished epoch 8, latest trainloss 0.0319, testloss 0.0529, test accuracy 0.9903\n",
      "Finished epoch 9, latest trainloss 0.0305, testloss 0.0537, test accuracy 0.9904\n",
      "Finished epoch 10, latest trainloss 0.0293, testloss 0.054, test accuracy 0.9903\n",
      "Finished epoch 11, latest trainloss 0.0282, testloss 0.0553, test accuracy 0.9896\n",
      "Finished epoch 12, latest trainloss 0.0273, testloss 0.0552, test accuracy 0.9901\n",
      "Finished epoch 13, latest trainloss 0.0265, testloss 0.0565, test accuracy 0.9892\n",
      "Finished epoch 14, latest trainloss 0.0259, testloss 0.0571, test accuracy 0.9894\n",
      "Finished epoch 15, latest trainloss 0.0252, testloss 0.057, test accuracy 0.9895\n",
      "Finished epoch 16, latest trainloss 0.0246, testloss 0.0584, test accuracy 0.9885\n",
      "Finished epoch 17, latest trainloss 0.0242, testloss 0.0587, test accuracy 0.9885\n",
      "Finished epoch 18, latest trainloss 0.0238, testloss 0.0589, test accuracy 0.9887\n",
      "Finished epoch 19, latest trainloss 0.0235, testloss 0.0599, test accuracy 0.9881\n"
     ]
    }
   ],
   "source": [
    "final_train = train_epochs(n_epochs=20, learning_rate=0.0001, weight_decay=0.001) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"domain12_nn.mdl\", \"wb\") as f:\n",
    "    pkl.dump( [tfidf_vectorizer, selector, model], f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
