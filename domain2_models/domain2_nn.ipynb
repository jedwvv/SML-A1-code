{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.base import TransformerMixin\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs = 1, n_layers = 1, n_neurons_per_layer = None, leaky_slope = 0.01):\n",
    "        if n_layers > 0:\n",
    "            try:\n",
    "                assert n_neurons_per_layer is not None\n",
    "                assert len(n_neurons_per_layer) == n_layers\n",
    "            except Exception as e:\n",
    "                raise ValueError(\"provide numbers of neurons per layer defined by n_layers\")\n",
    "        \n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        if n_layers >= 1:\n",
    "            stack = [ nn.Linear( n_inputs, n_neurons_per_layer[0] ) ]\n",
    "            for k in range(n_layers-1):\n",
    "                stack += [ nn.LeakyReLU(leaky_slope), nn.Linear(n_neurons_per_layer[k], n_neurons_per_layer[k+1]) ]\n",
    "            stack += [ nn.LeakyReLU(leaky_slope), nn.Linear(n_neurons_per_layer[n_layers-1], n_outputs) ]\n",
    "        else:\n",
    "            stack = [ nn.Linear( n_inputs, n_outputs ) ]\n",
    "        \n",
    "        self.linear_relu_stack = nn.Sequential( *stack )\n",
    "        self.prob_predictor = nn.Sigmoid()\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        for name, param in self.named_parameters():\n",
    "            param.data.uniform_(-0.5,0.5)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "    def predict(self, x):\n",
    "        logits = self.forward(x)\n",
    "        return self.prob_predictor(logits)\n",
    "    \n",
    "    def loss_fn(self, x, y):\n",
    "        logits = self.forward(x)\n",
    "        return self.loss(logits, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(filepath):\n",
    "    with open(filepath, \"r\") as f:\n",
    "        dataset = [ json.loads(line, parse_int = str) for line in f ]\n",
    "    return dataset\n",
    "\n",
    "def get_vectorizer( texts: list[str], *, method=\"countvectorize\", **kwargs ) -> CountVectorizer:\n",
    "    \"\"\"From a list of texts, output an appropriate vectorizer either using CountVectorizer or TF-IDF depending on method argument. \n",
    "\n",
    "     Args:\n",
    "         texts (list[str]): list of strings, each item corresponding to a text.\n",
    "         method (str, optional): Method to select features. Defaults to \"count-vectorizer\".\n",
    "         **kwargs: kwarg arguments to pass to Vectorizer classes of sklearn.\n",
    "    Raises:\n",
    "        ValueError: If passing an non-specified method of text feature extraction\n",
    "\n",
    "     Returns:\n",
    "         pd.DataFrame: dataframe of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    #We want single digits to be tokenized. This regex considers everything as a token except whitespace.\n",
    "    kwargs['token_pattern'] = r'\\S+' \n",
    "    if method == \"countvectorize\":\n",
    "        vectorizer = CountVectorizer(**kwargs)\n",
    "    elif method == \"tfidf\":\n",
    "        vectorizer = TfidfVectorizer(**kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"{method} is not a supported method.\")\n",
    "    #Use texts to initialize vocabulary of vectorizer\n",
    "    vectorizer.fit(texts)\n",
    "    return vectorizer\n",
    "\n",
    "def sentencify(text: list) -> str:\n",
    "    sentence = \" \".join(text) \n",
    "    return sentence\n",
    "\n",
    "def loss(clf, X, y):\n",
    "    probs = clf.predict_log_proba(X)\n",
    "    y0 = probs[:,1]\n",
    "    y1 = probs[:,0]\n",
    "    loss = -y*y0 - (1-y)*y1\n",
    "    loss = loss.sum()/y.size\n",
    "    return loss\n",
    "\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = load_dataset(\"domain2_train_data.json\")\n",
    "datatexts_2 = [ sentencify(instance['text']) for instance in dataset_2 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no features: 91931\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = get_vectorizer( texts = datatexts_2,\n",
    "                                    method='tfidf',\n",
    "                                    use_idf=True,\n",
    "                                    ngram_range=(1,3),\n",
    "                                    max_df=0.995, #Ignore vocabulary appearing too frequently, probably words like \"is\", \"are\", \"and\", \"this\" etc.\n",
    "                                    min_df=10, #Ignore vocabulary that is too infrequent, as this may lead to low prediction accuracy.\n",
    "                                    )\n",
    "print(f\"no features: {tfidf_vectorizer.get_feature_names_out().size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13000, 91933)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = tfidf_vectorizer.transform( datatexts_2 ).toarray()\n",
    "y = np.array([1]*1500 + [0]*11500)\n",
    "lengths = np.zeros( (13000, 2) )\n",
    "for i in range(13000):\n",
    "    for k in range(2):\n",
    "        lengths[i,k] = len(dataset_2[i]['text'])**(k+1)\n",
    "lengths[:,0] /= lengths.max(axis=0)[0]\n",
    "lengths[:,1] /= lengths.max(axis=0)[1]\n",
    "X2 = np.hstack( (X2, lengths) )\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27241"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_clf = LogisticRegression(C=0.1, random_state=0)\n",
    "selector_clf.fit(X2, y)\n",
    "selector = SelectFromModel(selector_clf, prefit=True)\n",
    "selector.fit(X2, y)\n",
    "selector.get_feature_names_out().size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_selected = selector.transform(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X2_selected.astype(float), y.astype(float),\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=1,\n",
    "                                                    stratify=y\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().reshape(-1,1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().reshape(-1,1)\n",
    "X_train = X_train.to(mps_device)\n",
    "y_train = y_train.to(mps_device)\n",
    "X_test = X_test.to(mps_device)\n",
    "y_test = y_test.to(mps_device)\n",
    "train_dataloader = DataLoader(list(zip(X_train,y_train)), shuffle=True, batch_size=X_train.shape[0]//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=27241, out_features=500, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=500, out_features=1, bias=True)\n",
       "  )\n",
       "  (prob_predictor): Sigmoid()\n",
       "  (loss): BCEWithLogitsLoss()\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = NeuralNetwork( X_train.shape[1], 1, 1, [500] )\n",
    "model.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epochs(n_epochs, learning_rate, weight_decay):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "    for epoch in range(n_epochs):\n",
    "        mean_trainloss = 0\n",
    "        num_trainbatches = len(train_dataloader)\n",
    "        \n",
    "        #Begin training\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss_fn(X_batch, y_batch)\n",
    "            mean_trainloss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        mean_trainloss /= num_trainbatches\n",
    "        train_losses += [ mean_trainloss ]\n",
    "        \n",
    "        #Begin evaluation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            testloss = model.loss_fn(X_test, y_test).item()\n",
    "            test_losses += [testloss]\n",
    "            y_pred = model.predict(X_test).round()\n",
    "            y_pred = y_pred.cpu().detach().numpy().reshape(-1)\n",
    "            accuracy = balanced_accuracy_score(y_test.cpu().detach().numpy().reshape(-1), y_pred)\n",
    "            test_accuracy += [accuracy]\n",
    "            \n",
    "            y_pred = model.predict(X_train).round()\n",
    "            y_pred = y_pred.cpu().detach().numpy().reshape(-1)\n",
    "            accuracy = balanced_accuracy_score(y_train.cpu().detach().numpy().reshape(-1), y_pred)\n",
    "            train_accuracy += [accuracy]\n",
    "        print(f'Finished epoch {epoch}, latest trainloss {np.round(mean_trainloss,4)}, testloss {np.round( testloss, 4 )}, test accuracy {np.round(test_accuracy[-1],4)}')\n",
    "    return train_losses, test_losses, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 3.303, testloss 0.3848, test accuracy 0.6414\n",
      "Finished epoch 1, latest trainloss 0.1229, testloss 0.1132, test accuracy 0.9474\n",
      "Finished epoch 2, latest trainloss 0.0197, testloss 0.166, test accuracy 0.8125\n",
      "Finished epoch 3, latest trainloss 0.0052, testloss 0.1104, test accuracy 0.8912\n",
      "Finished epoch 4, latest trainloss 0.0017, testloss 0.098, test accuracy 0.9093\n",
      "Finished epoch 5, latest trainloss 0.0007, testloss 0.1164, test accuracy 0.8801\n",
      "Finished epoch 6, latest trainloss 0.0006, testloss 0.1116, test accuracy 0.8966\n",
      "Finished epoch 7, latest trainloss 0.0004, testloss 0.1077, test accuracy 0.9047\n",
      "Finished epoch 8, latest trainloss 0.0004, testloss 0.1084, test accuracy 0.9047\n",
      "Finished epoch 9, latest trainloss 0.0003, testloss 0.1104, test accuracy 0.9047\n"
     ]
    }
   ],
   "source": [
    "trains += [ train_epochs(n_epochs=10, learning_rate=0.1, weight_decay=0.0) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 0.0003, testloss 0.1063, test accuracy 0.903\n",
      "Finished epoch 1, latest trainloss 0.0003, testloss 0.1025, test accuracy 0.9012\n",
      "Finished epoch 2, latest trainloss 0.0004, testloss 0.0989, test accuracy 0.9012\n",
      "Finished epoch 3, latest trainloss 0.0004, testloss 0.0957, test accuracy 0.9012\n",
      "Finished epoch 4, latest trainloss 0.0005, testloss 0.0927, test accuracy 0.9012\n",
      "Finished epoch 5, latest trainloss 0.0006, testloss 0.0901, test accuracy 0.9012\n",
      "Finished epoch 6, latest trainloss 0.0007, testloss 0.0877, test accuracy 0.9012\n",
      "Finished epoch 7, latest trainloss 0.0009, testloss 0.0857, test accuracy 0.8962\n",
      "Finished epoch 8, latest trainloss 0.001, testloss 0.0839, test accuracy 0.8962\n",
      "Finished epoch 9, latest trainloss 0.0013, testloss 0.0825, test accuracy 0.8926\n"
     ]
    }
   ],
   "source": [
    "trains += [ train_epochs(n_epochs=10, learning_rate=0.001, weight_decay=0.01) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 0.0014, testloss 0.083, test accuracy 0.8928\n",
      "Finished epoch 1, latest trainloss 0.0014, testloss 0.0834, test accuracy 0.8928\n",
      "Finished epoch 2, latest trainloss 0.0013, testloss 0.0839, test accuracy 0.8928\n",
      "Finished epoch 3, latest trainloss 0.0013, testloss 0.0843, test accuracy 0.8928\n",
      "Finished epoch 4, latest trainloss 0.0013, testloss 0.0848, test accuracy 0.8928\n",
      "Finished epoch 5, latest trainloss 0.0013, testloss 0.0851, test accuracy 0.8912\n",
      "Finished epoch 6, latest trainloss 0.0013, testloss 0.0855, test accuracy 0.8912\n",
      "Finished epoch 7, latest trainloss 0.0013, testloss 0.0858, test accuracy 0.8912\n",
      "Finished epoch 8, latest trainloss 0.0013, testloss 0.0861, test accuracy 0.8912\n",
      "Finished epoch 9, latest trainloss 0.0013, testloss 0.0864, test accuracy 0.8914\n"
     ]
    }
   ],
   "source": [
    "trains += [train_epochs(n_epochs=10, learning_rate=0.0001, weight_decay=0.001)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now reshuffle the data and re-train as it is likely overfitting to the current training data\n",
    "X_train, X_test, y_train, y_test = train_test_split( X2_selected.astype(float), y.astype(float),\n",
    "                                                    test_size=0.2,\n",
    "                                                    shuffle=True,\n",
    "                                                    random_state=2,\n",
    "                                                    stratify=y\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float().reshape(-1,1)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float().reshape(-1,1)\n",
    "X_train = X_train.to(mps_device)\n",
    "y_train = y_train.to(mps_device)\n",
    "X_test = X_test.to(mps_device)\n",
    "y_test = y_test.to(mps_device)\n",
    "train_dataloader = DataLoader(list(zip(X_train,y_train)), shuffle=True, batch_size=X_train.shape[0]//10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaltrain = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest trainloss 0.0185, testloss 0.0171, test accuracy 0.9831\n",
      "Finished epoch 1, latest trainloss 0.0183, testloss 0.0169, test accuracy 0.9831\n",
      "Finished epoch 2, latest trainloss 0.018, testloss 0.0167, test accuracy 0.9831\n",
      "Finished epoch 3, latest trainloss 0.0178, testloss 0.0166, test accuracy 0.9831\n",
      "Finished epoch 4, latest trainloss 0.0177, testloss 0.0164, test accuracy 0.9831\n",
      "Finished epoch 5, latest trainloss 0.0175, testloss 0.0163, test accuracy 0.9831\n",
      "Finished epoch 6, latest trainloss 0.0173, testloss 0.0161, test accuracy 0.9831\n",
      "Finished epoch 7, latest trainloss 0.0171, testloss 0.016, test accuracy 0.9831\n",
      "Finished epoch 8, latest trainloss 0.017, testloss 0.0159, test accuracy 0.9831\n",
      "Finished epoch 9, latest trainloss 0.0169, testloss 0.0158, test accuracy 0.9831\n"
     ]
    }
   ],
   "source": [
    "finaltrain += [ train_epochs(n_epochs=10, learning_rate=0.0001, weight_decay=0.001) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open(\"domain2_nn.mdl\", \"wb\") as f:\n",
    "    pkl.dump( [tfidf_vectorizer, selector_clf, selector, model], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
